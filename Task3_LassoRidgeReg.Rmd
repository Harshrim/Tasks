---
title: "Taask3_LassoRidgeReg"
author: "Harshrim Pardal"
date: "6/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(readxl)
library(DataExplorer)
data1<-read_xlsx("D:\\RIMA\\MBA\\Christ\\SIP-Outlook\\Data\\MRA Analytics.xlsx",sheet=1)
str(data1)
summary(data1)
dim(data1)
data1<-na.omit(data1) #removing the records containing null values
dim(data1)
```

```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(lmtest)
#library(estimatr)
library(car) #vif
library(caret)

```

**MODEL DEVELOPMENT**

```{r}
library(caTools)
#Creating a subset of 70% data
set.seed(100)
split1<-sample.split(data1$Id, SplitRatio=0.7)
data1train<-subset(data1, split1==TRUE)
data1test<-subset(data1, split1==FALSE)

```
**RIDGE REGRESSION:**

Linear regression algorithm works by selecting coefficients for each independent variable that minimizes a loss function. However, if the coefficients are large, they can lead to over-fitting on the training dataset, and such a model will not generalize well on the unseen test data. To overcome this shortcoming, we'll do regularization, which penalizes large coefficients. The glmnet() package will be used  to build the regularized regression models. The glmnet function does not work with dataframes, so we need to create a numeric matrix for the training features and a vector of target values.

The lines of code below perform the task of creating model matrix using the dummyVars function from the caret package. The predict function is then applied to create numeric model matrices for training and test.

Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients.

Loss function = OLS + alpha * summation (squared coefficient values)

Ridge regression is also referred to as l2 regularization

The arguments used in the model are:

nlambda: determines the number of regularization parameters to be tested.

alpha: determines the weighting to be used. In case of ridge regression, the value of alpha is zero.

family: determines the distribution family to be used. Since this is a regression model, we will use the Gaussian distribution.

lambda: determines the lambda values to be tried.

```{r}
#Ridge
cols_reg = c('Price', 'Age', 'KM', 'HP', 'cc','Doors','Quarterly_Tax','Weight')

dummies <- dummyVars(Price ~ ., data = data1[,cols_reg])

train_dummies = predict(dummies, newdata = data1train[,cols_reg])

test_dummies = predict(dummies, newdata = data1test[,cols_reg])

print(dim(train_dummies)); print(dim(test_dummies))

library(glmnet)

x = as.matrix(train_dummies)
y_train = data1train$Price

x_test = as.matrix(test_dummies)
y_test = data1test$Price

lambdas <- 10^seq(2, -3, by = -.1)
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = lambdas)

summary(ridge_reg)

cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda


# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))

  
  # Model performance metrics
data.frame(
  RMSE = RMSE,
  Rsquare = R_square
)
  
}

# Prediction and evaluation on train data
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x)
eval_results(y_train, predictions_train, data1train)

# Prediction and evaluation on test data
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
eval_results(y_test, predictions_test, data1test)

```
**LASSO REGRESSION:**

Lasso regression, or the Least Absolute Shrinkage and Selection Operator, is also a modification of linear regression. In lasso, the loss function is modified to minimize the complexity of the model by limiting the sum of the absolute values of the model coefficients (also called the l1-norm).

The loss function for lasso regression can be expressed as below:

Loss function = OLS + alpha * summation (absolute values of the magnitude of the coefficients)

In the above function, alpha is the penalty parameter we need to select. Using an l1-norm constraint forces some weight values to zero to allow other coefficients to take non-zero values.

The first step to build a lasso model is to find the optimal lambda value using the code below. For lasso regression, the alpha value is 1. The output is the best cross-validated lambda, which comes out to be 0.001.
```{r}
lambdas <- 10^seq(2, -3, by = -.1)

# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)

# Best 
lambda_best <- lasso_reg$lambda.min 
lambda_best

lasso_model <- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE)

predictions_train <- predict(lasso_model, s = lambda_best, newx = x)
eval_results(y_train, predictions_train, data1train)

predictions_test <- predict(lasso_model, s = lambda_best, newx = x_test)
eval_results(y_test, predictions_test, data1test)
```